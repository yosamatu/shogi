{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Alpha Zero General.\n",
    "This implementation is based on https://github.com/suragnair/alpha-zero-general/blob/master/Coach.py\n",
    "Python shogi library is here https://github.com/gunyarakun/python-shogi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    \"\"\"\n",
    "    This class specifies the base Game class. To define your own game, subclass\n",
    "    this class and implement the functions below. This works when the game is\n",
    "    two-player, adversarial and turn-based.\n",
    "    Use 1 for player1 and -1 for player2.\n",
    "    See othello/OthelloGame.py for an example implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def getInitBoard(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            startBoard: a representation of the board (ideally this is the form\n",
    "                        that will be the input to your neural network)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def getBoardSize(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (x,y): a tuple of board dimensions\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def getActionSize(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            actionSize: number of all possible actions\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def getNextState(self, board, player, action):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player (1 or -1)\n",
    "            action: action taken by current player\n",
    "        Returns:\n",
    "            nextBoard: board after applying action\n",
    "            nextPlayer: player who plays in the next turn (should be -player)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def getValidMoves(self, board, player):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player\n",
    "        Returns:\n",
    "            validMoves: a binary vector of length self.getActionSize(), 1 for\n",
    "                        moves that are valid from the current board and player,\n",
    "                        0 for invalid moves\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def getGameEnded(self, board, player):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player (1 or -1)\n",
    "        Returns:\n",
    "            r: 0 if game has not ended. 1 if player won, -1 if player lost,\n",
    "               small non-zero value for draw.\n",
    "               \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def getCanonicalForm(self, board, player):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player (1 or -1)\n",
    "        Returns:\n",
    "            canonicalBoard: returns canonical form of board. The canonical form\n",
    "                            should be independent of player. For e.g. in chess,\n",
    "                            the canonical form can be chosen to be from the pov\n",
    "                            of white. When the player is white, we can return\n",
    "                            board as is. When the player is black, we can invert\n",
    "                            the colors and return the board.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def getSymmetries(self, board, pi):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            pi: policy vector of size self.getActionSize()\n",
    "        Returns:\n",
    "            symmForms: a list of [(board,pi)] where each tuple is a symmetrical\n",
    "                       form of the board and the corresponding pi vector. This\n",
    "                       is used when training the neural network from examples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def stringRepresentation(self, board):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "        Returns:\n",
    "            boardString: a quick conversion of board to a string format.\n",
    "                         Required by MCTS for hashing.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " l  n  s  g  k  g  s  n  l\n",
      " .  r  .  .  .  .  .  b  .\n",
      " p  p +P  p  p  p  p  p  p\n",
      " .  .  .  .  .  .  .  .  .\n",
      " .  .  .  .  .  .  .  .  .\n",
      " .  .  .  .  .  .  .  .  .\n",
      " P  P  .  P  P  P  P  P  P\n",
      " .  B  .  .  .  .  .  R  .\n",
      " L  N  S  G  K  G  S  N  L\n",
      "\n",
      " P*1\n",
      "28\n",
      "[[0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import shogi\n",
    "import numpy as np\n",
    "\n",
    "NUM_BOARD_CHANNEL = 28\n",
    "NUM_PRISONER_CHANNEL = 14\n",
    "NUM_REPETITION_CHANNEL = 3\n",
    "NUM_BOARD_SIZE = 9\n",
    "\n",
    "NUM_PRISONER = 7\n",
    "\n",
    "class ShogiGame(Game):\n",
    "    def __init__(self):\n",
    "        self.board = shogi.Board()\n",
    "    \n",
    "    \n",
    "    def getInitState(self):\n",
    "        # return initialized state\n",
    "        self.board.reset()\n",
    "        return self.getState()\n",
    "        \n",
    "    \n",
    "    def getStateShape(self):\n",
    "        return (NUM_BOARD_CHANNEL + NUM_PRISONER_CHANNEL + NUM_REPETITION_CHANNEL, NUM_BOARD_SIZE, NUM_BOARD_SIZE)\n",
    "    \n",
    "    \n",
    "    def getNextState(self, current_player, tensor_action):\n",
    "        # TODO : DECIDE the representation of actions to aplly it into usi forms\n",
    "        \n",
    "        # string_action = self.getStringAction(tensor_action)\n",
    "        # self.board.push(shogi.Move.from_usi(string_action)\n",
    "        # return self.getState()\n",
    "        \"\"\"\n",
    "            The action output of neural net is represented as 3-D tensor like [CHANNEL,ROW,COL] filled with possibility of each action.\n",
    "            The representation is divided into two way.\n",
    "            For first (64 + 2) * 2(For Promotion) CHANNELs, CHANNEL specify the Movement(QUEEN MOVE + KNIGHT MOVE). And other 7 CHANNELs, CHANNEL corresponds to prisoner.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "    def getStringAction(self, tensor_action):\n",
    "        \"\"\"\n",
    "        args tensor_action :3-D tensor like [CHANNEL, ROW, COL]\n",
    "        return :action(like \"7g7f\")\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def getState(self, current_player=shogi.BLACK):\n",
    "        \"\"\"\n",
    "        return: State as binary expression with numpy 3-dimensional array.\n",
    "                First dimension stands for channels which corresponds to piece_type, \n",
    "                prisoners or special rules like repetation\n",
    "                Second is for rows and third is for columns.\n",
    "                \n",
    "        \"\"\"\n",
    "        # First dimension size is 28 for pieces on the board, 14 for prisoners, 3 for repetision\n",
    "        state = np.zeros(self.getStateShape(), dtype=np.int8)\n",
    "        \n",
    "        # BOARD STATE (from 0 to 27 CHANNELS)\n",
    "        for cell in shogi.SQUARES:\n",
    "            piece = self.board.piece_at(cell)\n",
    "            if piece:\n",
    "                # calculate channel number for the piece_type\n",
    "                channel = piece.color * len(shogi.PIECE_TYPES) + (piece.piece_type - 1)\n",
    "                # NOTE: col index starts from right to left. \n",
    "                row = cell // 9\n",
    "                col = cell - row * 9\n",
    "                state[channel,row,col] = 1\n",
    "\n",
    "        # PRISONER CHANNEL (from 28 to 41 CHANNELS)\n",
    "        prison = self.board.pieces_in_hand\n",
    "        for color in shogi.COLORS:\n",
    "            for piece_type in range(1, NUM_PRISONER + 1):\n",
    "                channel = NUM_BOARD_CHANNEL + color * NUM_PRISONER + (piece_type - 1) \n",
    "                if prison[color][piece_type]:\n",
    "                    print(channel)\n",
    "                    state[channel,0,:prison[color][piece_type]] = 1\n",
    "                \n",
    "        # REPETITION CHANNEL (from 42 to 44 CHANNELS)\n",
    "        repetition = self.board.transpositions[self.board.zobrist_hash()] - 1\n",
    "        for repeat in range(repetition):\n",
    "            state[NUM_BOARD_CHANNEL + NUM_PRISONER_CHANNEL + repeat] += 1\n",
    "            if repeat > 4:\n",
    "                print(\"GAME IS OVER\")\n",
    "                sys.exit(1)\n",
    "        if current_player:\n",
    "            state = self.changePerspective(state)\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def changePerspective(self, state):\n",
    "        # return reversed state to represent it from opponent's perspective\n",
    "        \n",
    "        # reverse the perspective of board\n",
    "        state[:NUM_BOARD_CHANNEL,:,:] = state[:NUM_BOARD_CHANNEL,::-1,::-1]\n",
    "        temp = state[:len(shogi.PIECE_TYPES),:,:]\n",
    "        state[:len(shogi.PIECE_TYPES),:,:] = state[len(shogi.PIECE_TYPES):NUM_BOARD_CHANNEL,:,:]\n",
    "        state[len(shogi.PIECE_TYPES):NUM_BOARD_CHANNEL,:,:] = temp\n",
    "        \n",
    "        # reverse prisonors information\n",
    "        temp = state[NUM_BOARD_CHANNEL:NUM_BOARD_CHANNEL + NUM_PRISONER,:,:]\n",
    "        state[NUM_BOARD_CHANNEL:NUM_BOARD_CHANNEL + NUM_PRISONER,:,:] = state[NUM_BOARD_CHANNEL + NUM_PRISONER:NUM_BOARD_CHANNEL + NUM_PRISONER_CHANNEL]\n",
    "        state[NUM_BOARD_CHANNEL + NUM_PRISONER:NUM_BOARD_CHANNEL + NUM_PRISONER_CHANNEL] = temp\n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.board.__str__()\n",
    "        \n",
    "game = ShogiGame()\n",
    "game.board.push(shogi.Move.from_usi(\"7g7c+\"))\n",
    "print(game)\n",
    "print(game.getState(current_player=shogi.BLACK)[28 + 14])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " l  n  s  g  k  g  s  n  l\n",
      " .  r  .  .  .  .  .  b  .\n",
      " p  p +P  p  p  p  p  p  p\n",
      " .  .  .  .  .  .  .  .  .\n",
      " .  .  .  .  .  .  .  .  .\n",
      " .  .  .  .  .  .  .  .  .\n",
      " P  P  .  P  P  P  P  P  P\n",
      " .  B  .  .  .  .  .  R  .\n",
      " L  N  S  G  K  G  S  N  L\n",
      "\n",
      " P*1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shogi\n",
    "class tempShogi():\n",
    "    def __init__(self,):\n",
    "        self.board = shogi.Board()\n",
    "        \n",
    "    def move(self, usi):\n",
    "        self.board.push(shogi.Move.from_usi(usi))\n",
    "        \n",
    "        \n",
    "    def __str__(self,):\n",
    "        return self.board.__str__()\n",
    "\n",
    "game = tempShogi()\n",
    "game.move(\"7g7c+\")\n",
    "print(game)\n",
    "game.board.piece_at(shogi.A5).piece_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "class Coach():\n",
    "    \"\"\"\n",
    "    This class executes the self-play + learning.\n",
    "    It uses the functions defined in Game and NeuralNet.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.pnet = self.nnet.__class__(self.game)\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
    "        self.trainExamplesHistory = []\n",
    "        self.skipFirstSelfPlay = False\n",
    "        \n",
    "    \n",
    "    def executeEpisode(self):\n",
    "        \"\"\"\n",
    "        This function executes one episode of self-play, starting with player 1.\n",
    "        As the game is played, each turn is added as a training example to trainExamples.\n",
    "        The game is played till the game ends.\n",
    "        AFter game ends the outcome of the game is used to assign values to each Example in trainExamples\n",
    "        \n",
    "        \n",
    "        It uses a temp(erature)=1 if episodeStep < tempThreshold, and thereafter uses temp=0\n",
    "        \n",
    "        Returns:\n",
    "            trainExamples: a list of examples of the form(canonicalBoard, pi, v)\n",
    "                            pi is the MCTS informed policy vector, v is +1 if the player won, else -1.\n",
    "        \"\"\"\n",
    "        \n",
    "        trainExamples = []\n",
    "        state = self.game.getInitState()\n",
    "        self.curPlayer = 0\n",
    "        episodeStep = 0\n",
    "        \n",
    "        while True:\n",
    "            episodeStep += 1\n",
    "            canonicalState = self.game.getCanonicalForm(state, self,curPlayer)\n",
    "            temp = int(episodeStep < self.args.tempThreshold)\n",
    "            \n",
    "            pi = self.mcts.getActionProb(canonicalState, temp=temp)\n",
    "            sym = self.game.getSymmetries(canonicalState, pi)\n",
    "            for s, p in sym:\n",
    "                trainExamples.append([s, self.curPlayer, p, None])\n",
    "                \n",
    "            action = np.random.choice(len(pi), p=pi)\n",
    "            state, self.curPlayer = self.game.getNextState(state, self.curPlayer, action)\n",
    "            \n",
    "            r = self.game.getGameEnded(state, self.curPlayer)\n",
    "            \n",
    "            if r!=0:\n",
    "                return [(x[0],x[2], r*((-1) ** (x[1]!=self.curPlayer))) for x in trainExamples]\n",
    "            \n",
    "            \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Performs numIters iterations with numEps episodes of self-play in each iteration.\n",
    "        After Every iteration, it retrains neural network with examples in trainExamples.\n",
    "        It then pits the new neural network against the old one and accepts it only if it wins >= updateThreshold fraction of games\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(1, self.args.numIter+1):\n",
    "            print('------ITER ' + str(i) + '------')\n",
    "            if not self.skipFirstSelfPlay or i>1:\n",
    "                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)\n",
    "                \n",
    "                for eps in range(self.args.numEps):\n",
    "                    # reset our MCTS by each episode.\n",
    "                    self.mcts = MCTS(self.game, self.nnet, self.args)\n",
    "                    iterationTrainExamples += self.executeEpisode()\n",
    "                    \n",
    "                # save the iteration examples to the history\n",
    "                self.trainExamplesHistory.append(iterationTrainExamples)\n",
    "            \n",
    "            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n",
    "                print(\"len(trainExamplesHistory) =\", len(self.trainExamplesHistory), \" => remove the oldest trainExamples\")\n",
    "                self.trainExampleHistory.pop(0)\n",
    "            # backup history to a file\n",
    "            # warning! the examples were collected using the model from the previous iteration, so (i-1)\n",
    "            self.saveTrainExamples(i-1)\n",
    "            \n",
    "            # shuffle examples before training\n",
    "            trainExamples = []\n",
    "            for e in self.trainExamplesHistory:\n",
    "                trainExamples.extend(e)\n",
    "            shuffle(trainExamples)\n",
    "            \n",
    "            # training new network, keeping a copy of the old one\n",
    "            self.nnet.save_chackpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "            self.pnet.load_chackpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "            pmcts = MCTS(self.game, self.pnet, self.args)\n",
    "            \n",
    "            self.nnet.train(trainExamples)\n",
    "            nmcts = MCTS(self.game, self.nnet, self.args)\n",
    "            \n",
    "            print('PITTING AGAINST PREVIOUS VERSION')\n",
    "            arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x, temp=0)),\n",
    "                          lambda x: np.argmax(nmcts.getActionProb(x, temp=0)), self.game)\n",
    "            pwins, nwins, draws = arena.playGames(self.args.arenaCompare)\n",
    "            print('NEW/PREV WINS : %d / %d ; DRAWS : %d' % (nwins, pwins, draws))\n",
    "            \n",
    "            if pwins+nwins > 0 and float(nwins)/(pwins+nwins) < self.args.updateThreshold:\n",
    "                print('REJECTING NEW MODEL')\n",
    "                self.nnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "            else:\n",
    "                print('ACCEPTING NEW MODEL')\n",
    "                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=self.getCheckpointFile(i))\n",
    "                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='best.pth.tar')\n",
    "                \n",
    "        \n",
    "    def getCheckpointFile(self, iteration):\n",
    "        return 'checkpoint_' + str(iteration) + '.pth.tar'\n",
    "            \n",
    "    \n",
    "    def saveTrainExamples(self, iteration):\n",
    "        folder = self.args.checkpoint\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        filename = os.path.join(folder, self.getCheckpointFile(iteration) + '.examples')\n",
    "        with open(filename, 'wb+') as f:\n",
    "            Pickler(f).dump(self.trainExamplesHistory)\n",
    "        f.closed\n",
    "        \n",
    "    \n",
    "    def loadTrainExamples(self):\n",
    "        modelFile = os.path.join(self.args.load_folder_file[0], self.args.load_folder_file[1])\n",
    "        examplesFile = modelFile + '.examples'\n",
    "        if not os.path.isfile(examplesFile):\n",
    "            print(examplesFile)\n",
    "            r = input(\"File with trainExamples not found. Continue? [y|n]\")\n",
    "            if r != \"y\":\n",
    "                sys.exit()\n",
    "        else:\n",
    "            print(\"File with trainExamples found. Read it.\")\n",
    "            with open(examplesFile, \"rb\") as f:\n",
    "                self.trainExamplesHistory = Unpickler(f).load()\n",
    "            f.closed\n",
    "\n",
    "            # examples based on the model were already collected (loaded)\n",
    "            self.skipFirstSelfPlay = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "EPS = 1e-8\n",
    "\n",
    "class MCTS():\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}\n",
    "        salf.Nsa = {}\n",
    "        self.Ns = {}\n",
    "        self.Ps = {}\n",
    "        \n",
    "        self.Es = {}\n",
    "        self.Vs = {}\n",
    "        \n",
    "    def getActionProb(self, canonicalState, temp=1):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from canonicalState.\n",
    "        \n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.args.numMCTSSims):\n",
    "            self.search(canonicalState)\n",
    "            \n",
    "        s = self.game.stringRepresentation(canonicalState)\n",
    "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "        if temp==0:\n",
    "            bestA = np.argmax(counts)\n",
    "            probs = [0] * len(counts)\n",
    "            probs[bestA] = 1\n",
    "            return probs\n",
    "        \n",
    "        counts = [x ** (1./temp) for x in counts]\n",
    "        probs = [x/float(sum(counts)) for x in counts]\n",
    "        return probs\n",
    "    \n",
    "    \n",
    "    def search(self, canonicalState):\n",
    "        \"\"\"\n",
    "        This function perfroms one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "        \n",
    "        Once a leaf node is found, the neural network is called to return an \n",
    "        initial policy P and a value V for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the \n",
    "        outcome is propagated up the search path. The value of Ns, Nsa, Qsa are \n",
    "        updated.\n",
    "        \n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1, 1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        \n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalState\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        s = self.game.stringRepresentation(canonicalState)\n",
    "        \n",
    "        if s not in self.Es.keys():\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalState, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # terminal state\n",
    "            return -self.Es[s]\n",
    "        \n",
    "        \n",
    "        if s not in self.Ps.keys():\n",
    "            \"\"\"\n",
    "            When you expand a leaf node\n",
    "            \"\"\"\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalState)\n",
    "            valids = self.game.getValidMoves(canonicalState, 1)\n",
    "            self.Ps[s] = self.Ps[s]*valids\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s\n",
    "            else:\n",
    "                \"\"\"\n",
    "                If all the valid moves were masked make all valid moves equally probable.\n",
    "                \n",
    "                NOTE: All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting\n",
    "                If you have got dozens or hundreds of these messages you should pay attention to your NNet and or training process\n",
    "                \"\"\"\n",
    "                print(\"All valid moves were masked, do wokaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "                \n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "            return -v\n",
    "        \n",
    "        \"\"\"\n",
    "        When you arrive at a leaf node which you have been to.\n",
    "        self.Vs[s] is already assigned because you have ever been to this node before.\n",
    "        \"\"\"\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "        \n",
    "        # pick the action with highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a] * math.sqrt(self.Ns[s]) / (1 + self.Nsa[(s, a)])\n",
    "                else:\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + EPS)\n",
    "                    \n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "                    \n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(canonicalState, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "        \n",
    "        v = self.search(next_s)\n",
    "        \n",
    "        if (s,a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "        else:\n",
    "            self.Qsa[(s,a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "            \n",
    "        self.Ns[s] += 1\n",
    "        return -v\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "tensor = torch.zeros(139,9,9)\n",
    "tensor[0][0][0] = 1\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
